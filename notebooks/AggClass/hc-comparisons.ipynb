{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "from time import time\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from rsq.samplers import *\n",
    "from rsq.samplers import _Sampler\n",
    "from rsq.helpers import set_seeds\n",
    "\n",
    "from rsq import SVCEnsemble, AgglomerativeEnsemble, AgglomerativeClassifier\n",
    "from rsq.agglomerative_helpers import get_tree_distances, get_decision_paths\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import pickle\n",
    "\n",
    "def stratified_sample(y, p=0.67, replace=False):\n",
    "    unique_y, counts = np.unique(y, return_counts=True)\n",
    "    n_per_class = np.array([int(np.math.floor(p*c)) for c in counts])\n",
    "    n_per_class = np.array([max([npc, 1]) for npc in n_per_class])\n",
    "    \n",
    "    inds = [np.random.choice(np.where(y == unique_y[i])[0], size=npc, replace=replace) for i, npc in enumerate(n_per_class)]\n",
    "    \n",
    "    return np.concatenate(inds)\n",
    "\n",
    "def few_shot_sample(y, n_samples_per_class=1):\n",
    "    unique_y = np.unique(y)    \n",
    "    inds = [np.random.choice(np.where(y == c)[0], size=n_samples_per_class, replace=False) for c in unique_y]\n",
    "    \n",
    "    return np.concatenate(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Data processing 1\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, file='cifar_100_Bit_m-r101x1_embd.p', train=True, classes=[]):\n",
    "        if train:\n",
    "            self.data = pickle.load(open(file, 'rb'))[0][0]\n",
    "            self.targets = np.concatenate(pickle.load(open(file, 'rb'))[0][1])\n",
    "        else:\n",
    "            self.data = pickle.load(open(file, 'rb'))[1][0]\n",
    "            self.targets = np.concatenate(pickle.load(open(file, 'rb'))[1][1])\n",
    "        \n",
    "        self.classes = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#- Data processing 2\n",
    "\n",
    "cif100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True)\n",
    "\n",
    "file='../../../../data/cifar_100_Bit_m-r101x1_embd.p'\n",
    "trainset = Dataset(file, train=True, classes=cif100.classes)\n",
    "testset = Dataset(file, train=False, classes=cif100.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_to_fine_map = {\n",
    "'aquatic_mammals': ['beaver', 'dolphin', 'otter', 'seal', 'whale'],\n",
    "'fish': ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],\n",
    "'flowers': ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],\n",
    "'food_containers': ['bottle', 'bowl', 'can', 'cup', 'plate'],\n",
    "'fruit_and_vegetables': ['apple', 'mushroom', 'orange', 'pear', 'sweet_pepper'],\n",
    "'household_electrical_devices': ['clock', 'keyboard', 'lamp', 'telephone', 'television'],\n",
    "'household_furniture': ['bed', 'chair', 'couch', 'table', 'wardrobe'],\n",
    "'insects': ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],\n",
    "'large_carnivores': ['bear', 'leopard', 'lion', 'tiger', 'wolf'],\n",
    "'large_man-made_outdoor_things': ['bridge', 'castle', 'house', 'road', 'skyscraper'],\n",
    "'large_natural_outdoor_scenes': ['cloud', 'forest', 'mountain', 'plain', 'sea'],\n",
    "'large_omnivores_and_herbivores': ['camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo'],\n",
    "'medium-sized_mammals': ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],\n",
    "'non-insect_invertebrates': ['crab', 'lobster', 'snail', 'spider', 'worm'],\n",
    "'people': ['baby', 'boy', 'girl', 'man', 'woman'],\n",
    "'reptiles': ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],\n",
    "'small mammals': ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],\n",
    "'trees': ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree', 'willow_tree'],\n",
    "'vehicles_1': ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train'],\n",
    "'vehicles_2': ['lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor']\n",
    "}\n",
    "\n",
    "coarse_number_to_coarse_name = {i: name for i, name in enumerate(coarse_to_fine_map)}\n",
    "\n",
    "def fine_to_coarse(coarse_to_fine):\n",
    "    fine_to_coarse_map = {}\n",
    "    for key in coarse_to_fine:\n",
    "        fines = coarse_to_fine[key]\n",
    "        for f in fines:\n",
    "            fine_to_coarse_map[f] = key\n",
    "            \n",
    "    return fine_to_coarse_map\n",
    "\n",
    "fine_to_coarse_map = fine_to_coarse(coarse_to_fine_map)\n",
    "\n",
    "fine_number_to_fine_name = {i: name for i, name in enumerate(trainset.classes)}\n",
    "fine_name_to_fine_number = {name: i for i, name in fine_number_to_fine_name.items()}\n",
    "\n",
    "for i in range(100):\n",
    "    fine_to_coarse_map[fine_number_to_fine_name[i]]\n",
    "    \n",
    "coarse_name_to_coarse_number = {name: i for i, name in enumerate(coarse_to_fine_map)}\n",
    "\n",
    "coarse_targets = np.array([coarse_name_to_coarse_number[fine_to_coarse_map[fine_number_to_fine_name[y]]] for y in trainset.targets])\n",
    "idx_by_coarse = np.array([np.where(coarse_targets == y)[0] for y in range(20)])\n",
    "idx_by_fine = np.array([np.where(trainset.targets == y)[0] for y in range(100)])\n",
    "\n",
    "\n",
    "test_coarse_targets = np.array([coarse_name_to_coarse_number[fine_to_coarse_map[fine_number_to_fine_name[y]]] for y in testset.targets])\n",
    "test_idx_by_coarse = np.array([np.where(test_coarse_targets == y)[0] for y in range(20)])\n",
    "\n",
    "\n",
    "coarse_names = np.array(list(coarse_name_to_coarse_number.keys()))\n",
    "\n",
    "fine_number_to_coarse_number = {fn: coarse_name_to_coarse_number[\n",
    "                                        fine_to_coarse_map[\n",
    "                                            fine_number_to_fine_name[fn]\n",
    "                                        ]\n",
    "                                    ] for fn in range(100)}\n",
    "\n",
    "\n",
    "fine_by_coarse = [np.where(np.array(list(fine_number_to_coarse_number.values())) == i)[0] for i in range(20)]\n",
    "all_fine = np.concatenate(fine_by_coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def ssl_exp(X, y, n_samples_per_class=1, acorn=None):    \n",
    "    inds_sup = few_shot_sample(y, n_samples_per_class)\n",
    "    inds_unsup = np.array([i for i in range(len(y)) if i not in inds_sup]).astype(int)\n",
    "    X=X.copy()\n",
    "\n",
    "    y_ = -1 * np.ones(n)\n",
    "    y_[inds_sup] = y[inds_sup]\n",
    "    \n",
    "    #- 2 = Supervised, Semisupervised\n",
    "    accs = np.zeros(2)\n",
    "    times = np.zeros(2)\n",
    "        \n",
    "    \n",
    "    print(\"begin fit2\")\n",
    "    #- Semi-Supervised\n",
    "    svc_semisup = AgglomerativeEnsemble(n_estimators=1, p_inbag=1, \n",
    "                                        projector=None, projection_kwargs={'n_components': 64}, \n",
    "                                        affinity='euclidean', linkage='average', max_tree_distance=400)\n",
    "    time_ = time()\n",
    "    svc_semisup.fit(X, y_)\n",
    "    \n",
    "    accs[1] = (svc_semisup.predict(X[inds_unsup]) == y[inds_unsup]).mean()\n",
    "    times[1] = time() - time_\n",
    "    \n",
    "    print(\"finish fit+predict 2 in\", times[1], \"seconds\")\n",
    "    time_ = time()\n",
    "    \n",
    "    \n",
    "    print(\"begin fit1\")\n",
    "    #- Supervised\n",
    "    svc_sup = LinearSVC(C=1)\n",
    "    time_ = time()\n",
    "    svc_sup.fit(X[inds_sup], y[inds_sup])\n",
    "    accs[0] = (svc_sup.predict(X[inds_unsup]) == y[inds_unsup]).mean()\n",
    "    times[0] = time() - time_\n",
    "    print(\"finish fit+predict 1 in\", times[0], \"seconds\")\n",
    "        \n",
    "    return accs, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "n_samples_per_class=[1]\n",
    "n_cores=40\n",
    "# n_mc=int(n_cores / len(prop_labeled))\n",
    "n_mc=1\n",
    "\n",
    "experiment_tuples = []\n",
    "\n",
    "for i, p in enumerate(n_samples_per_class):\n",
    "    for _ in range(n_mc):\n",
    "        all_inds = stratified_sample(trainset.targets, p=0.1, replace=False)\n",
    "        n=len(all_inds)\n",
    "        \n",
    "        experiment_tuples.append((trainset.data[all_inds], trainset.targets[all_inds], p))\n",
    "    \n",
    "\n",
    "condensed_func = lambda x: ssl_exp(*x)\n",
    "start_time = time()\n",
    "print(len(experiment_tuples))\n",
    "# try:\n",
    "#     accuracies_and_times = Parallel(n_jobs=n_cores)(delayed(condensed_func)(tupl) for tupl in experiment_tuples)\n",
    "#     print(\"finished in %1.1f\"%(time() - start_time))\n",
    "# except:\n",
    "#     print(\"error after %1.1f\"%(time() - start_time))\n",
    "#     assert 0 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,n_samples_per_class = experiment_tuples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import linkage as linkage_scipy\n",
    "from fastcluster import linkage as linkage_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_sup = few_shot_sample(y, n_samples_per_class)\n",
    "inds_unsup = np.array([i for i in range(len(y)) if i not in inds_sup]).astype(int)\n",
    "\n",
    "y_ = -1 * np.ones(n)\n",
    "y_[inds_sup] = y[inds_sup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.5 s, sys: 30.7 ms, total: 17.6 s\n",
      "Wall time: 17.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(linkage='average')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "agg = AgglomerativeClustering(affinity='euclidean', linkage='average')\n",
    "agg.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.2 s, sys: 33.9 ms, total: 17.2 s\n",
      "Wall time: 17.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.22200000e+03, 3.24400000e+03, 1.87435445e+01, 2.00000000e+00],\n",
       "       [1.20000000e+03, 1.20700000e+03, 2.21699063e+01, 2.00000000e+00],\n",
       "       [2.65400000e+03, 2.67700000e+03, 3.04658979e+01, 2.00000000e+00],\n",
       "       ...,\n",
       "       [8.21000000e+02, 9.99500000e+03, 1.39177685e+02, 4.99100000e+03],\n",
       "       [9.99400000e+03, 9.99600000e+03, 1.39483265e+02, 4.99900000e+03],\n",
       "       [4.18900000e+03, 9.99700000e+03, 1.41100420e+02, 5.00000000e+03]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "linkage_scipy(X, metric='euclidean',method='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.9 s, sys: 18.6 ms, total: 17.9 s\n",
      "Wall time: 17.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.22200000e+03, 3.24400000e+03, 1.87435445e+01, 2.00000000e+00],\n",
       "       [1.20000000e+03, 1.20700000e+03, 2.21699063e+01, 2.00000000e+00],\n",
       "       [2.65400000e+03, 2.67700000e+03, 3.04658979e+01, 2.00000000e+00],\n",
       "       ...,\n",
       "       [8.21000000e+02, 9.99500000e+03, 1.39177685e+02, 4.99100000e+03],\n",
       "       [9.99400000e+03, 9.99600000e+03, 1.39483265e+02, 4.99900000e+03],\n",
       "       [4.18900000e+03, 9.99700000e+03, 1.41100420e+02, 5.00000000e+03]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "linkage_fc(X, metric='euclidean',method='average')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "APU",
   "language": "python",
   "name": "apu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
